{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics\n",
    "1. Review of model evaluation procedures\n",
    "2. Steps for K-fold cross-validation\n",
    "3. Comparing cross-validation to train/test split\n",
    "4. Cross-validation recommendations\n",
    "5. Cross-validation example: parameter tuning\n",
    "6. Cross-validation example: model selection\n",
    "7. Cross-validation example: feature selection\n",
    "8. Improvements to cross-validation\n",
    "9. Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This tutorial is derived from Data School's Machine Learning with scikit-learn tutorial. I added my own notes so anyone, including myself, can refer to this tutorial without watching the videos._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Review of model evaluation procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation:** Need a way to choose between machine learning models\n",
    "\n",
    "- Goal is to estimate likely performance of a model on **out-of-sample data**\n",
    "\n",
    "**Initial idea:** Train and test on the same data\n",
    "\n",
    "- But, maximizing **training accuracy** rewards overly complex models which **overfit** the training data\n",
    "\n",
    "**Alternative idea:** Train/test split\n",
    "\n",
    "- Split the dataset into two pieces, so that the model can be trained and tested on **different data**\n",
    "- **Testing accuracy** is a better estimate than training accuracy of out-of-sample performance\n",
    "- Problem with train/test split\n",
    "    - It provides a **high variance** estimate since changing which observations happen to be in the testing set can significantly change testing accuracy\n",
    "    - Testing accuracy can change a lot depending on a which observation happen to be in the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in the iris data\n",
    "iris = load_iris()\n",
    "\n",
    "# create X (features) and y (response)\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97368421052631582"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use train/test split with different random_state values\n",
    "# we can change the random_state values that changes the accuracy scores\n",
    "# the accuracy changes a lot\n",
    "# this is why testing accuracy is a high-variance estimate\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=6)\n",
    "\n",
    "# check classification accuracy of KNN with K=5\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What if we created a bunch of train/test splits, calculated the testing accuracy for each, and averaged the results together?\n",
    "\n",
    "**Answer:** That's the essense of cross-validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Steps for K-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Split the dataset into K **equal** partitions (or \"folds\")\n",
    "    - So if k = 5 and dataset has 150 observations\n",
    "    - Each of the 5 folds would have 30 observations\n",
    "2. Use fold 1 as the **testing set** and the union of the other folds as the **training set**\n",
    "    - Testing set = 30 observations (fold 1)\n",
    "    - Training set = 120 observations (folds 2-5)\n",
    "3. Calculate **testing accuracy**\n",
    "4. Repeat steps 2 and 3 K times, using a **different fold** as the testing set each time\n",
    "    - We will repeat the process 5 times\n",
    "    - 2nd iteration\n",
    "        - fold 2 would be the testing set\n",
    "        - union of fold 1, 3, 4, and 5 would be the training set\n",
    "    - 3rd iteration\n",
    "        - fold 3 would be the testing set\n",
    "        - union of fold 1, 2, 4, and 5 would be the training set\n",
    "    - And so on...\n",
    "5. Use the **average testing accuracy** as the estimate of out-of-sample accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagram of **5-fold cross-validation:**\n",
    "\n",
    "![5-fold cross-validation](images/07_cross_validation_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration                   Training set obsevations                    Testing set observations\n",
      "    1     [ 5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]        [0 1 2 3 4]       \n",
      "    2     [ 0  1  2  3  4 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]        [5 6 7 8 9]       \n",
      "    3     [ 0  1  2  3  4  5  6  7  8  9 15 16 17 18 19 20 21 22 23 24]     [10 11 12 13 14]     \n",
      "    4     [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 20 21 22 23 24]     [15 16 17 18 19]     \n",
      "    5     [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]     [20 21 22 23 24]     \n"
     ]
    }
   ],
   "source": [
    "# simulate splitting a dataset of 25 observations into 5 folds\n",
    "from sklearn.cross_validation import KFold\n",
    "kf = KFold(25, n_folds=5, shuffle=False)\n",
    "\n",
    "\n",
    "# print the contents of each training and testing set\n",
    "# ^ - forces the field to be centered within the available space\n",
    "# .format() - formats the string similar to %s or %n\n",
    "# enumerate(sequence, start=0) - returns an enumerate object\n",
    "print('{} {:^61} {}'.format('Iteration', 'Training set obsevations', 'Testing set observations'))\n",
    "for iteration, data in enumerate(kf, start=1):\n",
    "    print('{!s:^9} {} {!s:^25}'.format(iteration, data[0], data[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset contains **25 observations** (numbered 0 through 24)\n",
    "- 5-fold cross-validation, thus it runs for **5 iterations**\n",
    "- For each iteration, every observation is either in the training set or the testing set, **but not both**\n",
    "- Every observation is in the testing set **exactly once**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Comparing cross-validation to train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages of **cross-validation:**\n",
    "\n",
    "- More accurate estimate of out-of-sample accuracy\n",
    "- More \"efficient\" use of data\n",
    "    - This is because every observation is used for both training and testing\n",
    "\n",
    "Advantages of **train/test split:**\n",
    "\n",
    "- Runs K times faster than K-fold cross-validation\n",
    "    - This is because K-fold cross-validation repeats the train/test split K-times\n",
    "- Simpler to examine the detailed results of the testing process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Cross-validation recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. K can be any number, but **K=10** is generally recommended\n",
    "    - This has been shown experimentally to produce the best out-of-sample estimate\n",
    "2. For classification problems, **stratified sampling** is recommended for creating the folds\n",
    "    - Each response class should be represented with equal proportions in each of the K folds\n",
    "        - If dataset has 2 response classes \n",
    "            - Spam/Ham\n",
    "            - 20% observation = ham\n",
    "            - Each cross-validation fold should consist of exactly 20% ham\n",
    "    - scikit-learn's `cross_val_score` function does this by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Cross-validation example: parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Select the best tuning parameters (aka \"hyperparameters\") for KNN on the iris dataset\n",
    "- We want to choose the best tuning parameters that best generalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.          0.93333333  1.          1.          0.86666667  0.93333333\n",
      "  0.93333333  1.          1.          1.        ]\n"
     ]
    }
   ],
   "source": [
    "# 10-fold cross-validation with K=5 for KNN (the n_neighbors parameter)\n",
    "# k = 5 for KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Use cross_val_score function\n",
    "# We are passing the entirety of X and y, not X_train or y_train, it takes care of splitting the dat\n",
    "# cv=10 for 10 folds\n",
    "# scoring='accuracy' for evaluation metric - althought they are many\n",
    "scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the first iteration, the accuracy is 100%\n",
    "- Second iteration, the accuracy is 93% and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cross_val_score executes the first 4 steps of k-fold cross-validation steps which I have broken down to 7 steps here in detail**\n",
    "1. Split the dataset (X and y) into K=10 **equal** partitions (or \"folds\")\n",
    "2. Train the KNN model on union of folds 2 to 10 (training set)\n",
    "3. Test the model on fold 1 (testing set) and calculate testing accuracy\n",
    "4. Train the KNN model on union of fold 1 and fold 3 to 10 (training set)\n",
    "5. Test the model on fold 2 (testing set) and calculate testing accuracy\n",
    "6. It will do this on 8 more times\n",
    "7. When finished, it will return the 10 testing accuracy scores as a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.966666666667\n"
     ]
    }
   ],
   "source": [
    "# use average accuracy as an estimate of out-of-sample accuracy\n",
    "# numpy array has a method mean()\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal here is to find the optimal value of K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95999999999999996, 0.95333333333333337, 0.96666666666666656, 0.96666666666666656, 0.96666666666666679, 0.96666666666666679, 0.96666666666666679, 0.96666666666666679, 0.97333333333333338, 0.96666666666666679, 0.96666666666666679, 0.97333333333333338, 0.98000000000000009, 0.97333333333333338, 0.97333333333333338, 0.97333333333333338, 0.97333333333333338, 0.98000000000000009, 0.97333333333333338, 0.98000000000000009, 0.96666666666666656, 0.96666666666666656, 0.97333333333333338, 0.95999999999999996, 0.96666666666666656, 0.95999999999999996, 0.96666666666666656, 0.95333333333333337, 0.95333333333333337, 0.95333333333333337]\n"
     ]
    }
   ],
   "source": [
    "# search for an optimal value of K for KNN\n",
    "\n",
    "# range of k we want to try\n",
    "k_range = range(1, 31)\n",
    "# empty list to store scores\n",
    "k_scores = []\n",
    "\n",
    "# 1. we will loop through reasonable values of k\n",
    "for k in k_range:\n",
    "    # 2. run KNeighborsClassifier with k neighbours\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    # 3. obtain cross_val_score for KNeighborsClassifier with k neighbours\n",
    "    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "    # 4. append mean of scores for k neighbors to k_scores list\n",
    "    k_scores.append(scores.mean())\n",
    "    \n",
    "\n",
    "print(k_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of list 30\n",
      "Max of list 0.98\n"
     ]
    }
   ],
   "source": [
    "# in essence, this is basically running the k-fold cross-validation method 30 times because we want to run through K values from 1 to 30\n",
    "# we should have 30 scores here\n",
    "print('Length of list', len(k_scores))\n",
    "print('Max of list', max(k_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x114d05908>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEPCAYAAACHuClZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114c51b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot how accuracy changes as we vary k\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# plot the value of K for KNN (x-axis) versus the cross-validated accuracy (y-axis)\n",
    "# plt.plot(x_axis, y_axis)\n",
    "plt.plot(k_range, k_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-validated accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum cv accuracy occurs from k=13 to k=20\n",
    "- The general shape of the curve is an upside down yield\n",
    "    - This is quite typical when examining the model complexity and accuracy\n",
    "    - This is an example of bias-variance trade off\n",
    "        - Low values of k (low bias, high variance)\n",
    "            - The 1-Nearest Neighbor classifier is the most complex nearest neighbor model\n",
    "            - It has the most jagged decision boundary, and is most likely to overfit\n",
    "        - High values of k (high bias, low variance) \n",
    "            - underfit\n",
    "        - Best value is the middle of k (most likely to generalize out-of-sample data)\n",
    "            - just right\n",
    "    \n",
    "- The best value of k\n",
    "    - Higher values of k produce less complex model\n",
    "        - So we will choose 20 as our best KNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Cross-validation example: model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Compare the best KNN model with logistic regression on the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98\n"
     ]
    }
   ],
   "source": [
    "# 10-fold cross-validation with the best KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=20)\n",
    "\n",
    "# Instead of saving 10 scores in object named score and calculating mean\n",
    "# We're just calculating the mean directly on the results\n",
    "print(cross_val_score(knn, X, y, cv=10, scoring='accuracy').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.953333333333\n"
     ]
    }
   ],
   "source": [
    "# 10-fold cross-validation with logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "print(cross_val_score(logreg, X, y, cv=10, scoring='accuracy').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that KNN is likely a better choice than logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Cross-validation example: feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal**: Select whether the Newspaper feature should be included in the linear regression model on the advertising dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in the advertising dataset\n",
    "data = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a Python list of three feature names\n",
    "feature_cols = ['TV', 'Radio', 'Newspaper']\n",
    "\n",
    "# use the list to select a subset of the DataFrame (X)\n",
    "X = data[feature_cols]\n",
    "\n",
    "# select the Sales column as the response (y)\n",
    "# since we're selecting only one column, we can select the attribute using .attribute\n",
    "y = data.Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.56038438 -3.29767522 -2.08943356 -2.82474283 -1.3027754  -1.74163618\n",
      " -8.17338214 -2.11409746 -3.04273109 -2.45281793]\n"
     ]
    }
   ],
   "source": [
    "# 10-fold cross-validation with all three features\n",
    "# instantiate model\n",
    "lm = LinearRegression()\n",
    "\n",
    "# store scores in scores object\n",
    "# we can't use accuracy as our evaluation metric since that's only relevant for classification problems\n",
    "# RMSE is not directly available so we will use MSE\n",
    "scores = cross_val_score(lm, X, y, cv=10, scoring='mean_squared_error')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE should be positive\n",
    "- But why is the MSE here negative?\n",
    "- MSE is a loss function\n",
    "    - It is something we want to minimize\n",
    "    - A design decision was made so that the results are made negative\n",
    "    - The best results would be the largest number (the least negative) so we can still maximize similar to classification accuracy\n",
    "- Classification Accuracy is a reward function\n",
    "    - It is something we want to maximize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.56038438  3.29767522  2.08943356  2.82474283  1.3027754   1.74163618\n",
      "  8.17338214  2.11409746  3.04273109  2.45281793]\n"
     ]
    }
   ],
   "source": [
    "# fix the sign of MSE scores\n",
    "mse_scores = -scores\n",
    "print(mse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.88689808  1.81595022  1.44548731  1.68069713  1.14139187  1.31971064\n",
      "  2.85891276  1.45399362  1.7443426   1.56614748]\n"
     ]
    }
   ],
   "source": [
    "# convert from MSE to RMSE\n",
    "rmse_scores = np.sqrt(mse_scores)\n",
    "print(rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.69135317081\n"
     ]
    }
   ],
   "source": [
    "# calculate the average RMSE\n",
    "print(rmse_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.67967484191\n"
     ]
    }
   ],
   "source": [
    "# 10-fold cross-validation with two features (excluding Newspaper)\n",
    "feature_cols = ['TV', 'Radio']\n",
    "X = data[feature_cols]\n",
    "print(np.sqrt(-cross_val_score(lm, X, y, cv=10, scoring='mean_squared_error')).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without Newspaper\n",
    "- Average RMSE = 1.68\n",
    "- lower number than with model with Newspaper\n",
    "    - RMSE is something we want to minimize\n",
    "    - So the model excluding Newspaper is a better model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Improvements to cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Repeated cross-validation**\n",
    "\n",
    "- Repeat cross-validation multiple times (with **different random splits** of the data) and average the results\n",
    "- More reliable estimate of out-of-sample performance by **reducing the variance** associated with a single trial of cross-validation\n",
    "\n",
    "**Creating a hold-out set**\n",
    "\n",
    "- \"Hold out\" a portion of the data **before** beginning the model building process\n",
    "- Locate the best model using cross-validation on the remaining data, and test it **using the hold-out set**\n",
    "- More reliable estimate of out-of-sample performance since hold-out set is **truly out-of-sample**\n",
    "\n",
    "**Feature engineering and selection within cross-validation iterations**\n",
    "\n",
    "- Normally, feature engineering and selection occurs **before** cross-validation\n",
    "- Instead, perform all feature engineering and selection **within each cross-validation iteration**\n",
    "- More reliable estimate of out-of-sample performance since it **better mimics** the application of the model to out-of-sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Resources\n",
    "\n",
    "- scikit-learn documentation: [Cross-validation](http://scikit-learn.org/stable/modules/cross_validation.html), [Model evaluation](http://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "- scikit-learn issue on GitHub: [MSE is negative when returned by cross_val_score](https://github.com/scikit-learn/scikit-learn/issues/2439)\n",
    "- Section 5.1 of [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/) (11 pages) and related videos: [K-fold and leave-one-out cross-validation](https://www.youtube.com/watch?v=nZAM5OXrktY) (14 minutes), [Cross-validation the right and wrong ways](https://www.youtube.com/watch?v=S06JpVoNaA0) (10 minutes)\n",
    "- Scott Fortmann-Roe: [Accurately Measuring Model Prediction Error](http://scott.fortmann-roe.com/docs/MeasuringError.html)\n",
    "- Machine Learning Mastery: [An Introduction to Feature Selection](http://machinelearningmastery.com/an-introduction-to-feature-selection/)\n",
    "- Harvard CS109: [Cross-Validation: The Right and Wrong Way](https://github.com/cs109/content/blob/master/lec_10_cross_val.ipynb)\n",
    "- Journal of Cheminformatics: [Cross-validation pitfalls when selecting and assessing regression and classification models](http://www.jcheminf.com/content/pdf/1758-2946-6-10.pdf)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
